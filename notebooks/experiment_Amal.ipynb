from torchvision import models
import torch.nn as nn

class EncoderInception(nn.Module):
    def __init__(self, encoded_size=14, fine_tune=True):
        super().__init__()
        # Load the pre-trained Inception-v3 model (without the auxiliary logits branch)
        inception = models.inception_v3(pretrained=True, aux_logits=False)
        # Remove the final fully-connected classification layer
        modules = list(inception.children())[:-1]
        # Keep only the convolutional backbone
        self.backbone = nn.Sequential(*modules)  # outputs feature maps of shape (B, 2048, H, W)
        
        # We’ll pool these feature maps down to a fixed spatial size
        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_size, encoded_size))
        
        # Freeze or unfreeze layers according to fine_tune flag
        self.fine_tune(fine_tune)

    def forward(self, x):
        # x: input images, shape (B, 3, H_img, W_img)
        # Pass through the Inception backbone to get feature maps
        x = self.backbone(x)                       # → (B, 2048, Hf, Wf)
        # Resize feature maps to (encoded_size × encoded_size)
        x = self.adaptive_pool(x)                  # → (B, 2048, enc, enc)
        
        # Flatten the spatial dimensions into a sequence of “image tokens”
        B, C, H, W = x.size()
        # View as (batch, channels, num_patches) then permute to (batch, num_patches, channels)
        x = x.view(B, C, H * W).permute(0, 2, 1)    # → (B, N=H·W, C)
        return x

    def fine_tune(self, fine_tune):
        # By default, freeze all backbone parameters so we don’t train them
        for p in self.backbone.parameters():
            p.requires_grad = False
        
        if fine_tune:
            # If fine-tuning, unfreeze only the last two inception blocks
            # (so we adapt high-level features but keep early layers fixed)
            for block in list(self.backbone.children())[-2:]:
                for p in block.parameters():
                    p.requires_grad = True
